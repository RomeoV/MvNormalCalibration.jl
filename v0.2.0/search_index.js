var documenterSearchIndex = {"docs":
[{"location":"api_reference/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api_reference/","page":"API Reference","title":"API Reference","text":"Modules = [MvNormalCalibration]","category":"page"},{"location":"api_reference/#MvNormalCalibration.computecalibration-Tuple{AbstractVector{<:Distributions.Normal}, AbstractVector{<:Real}}","page":"API Reference","title":"MvNormalCalibration.computecalibration","text":"computecalibration(preds::AbstractVector{<:Normal}, truevals::AbstractVector{<:Real}; pvals)\ncomputecalibration(preds::AbstractVector{<:MvNormal}, truevals::AbstractVector{<:AbstractVector{<:Real}}; pvals)\n\nCompute calibration for a series of predicted (uni- or multivariate) normal distributions given a series of true observations using central prediction sets.\n\nReturns a named tuple (; pvals, calibrationvals). If the predictions are well calibrated, then pvals ≈ calibrationvals for all indices. Plotting plot(pvals, calibrationvals) should then give a straight line from (0, 0), to (1, 1).\n\nkwargs\n\npvals: The probabilities to evaluate the coverage at. Defaults to 0:0.05:1.\n\n\n\n\n\n","category":"method"},{"location":"api_reference/#MvNormalCalibration.sharpness-Tuple{Distributions.Normal}","page":"API Reference","title":"MvNormalCalibration.sharpness","text":"sharpness(pred::Normal)\nsharpness(pred::AbstractMvNormal)\n\nCompute sharpness for (uni- or multivariate) normal distribution, which we define as the (hyper-)volume of the ellipsoid that contains one standard deviation of the distribution.\n\nExamples\n\njulia> sharpness(Normal())\n2\njulia> sharpness(MvNormal(zeros(2), I(2)))  # recall area of circle=πr^2\nπ\n\n\n\n\n\n","category":"method"},{"location":"mathematical_background/#Mathematical-Background","page":"Mathematical Background","title":"Mathematical Background","text":"","category":"section"},{"location":"mathematical_background/#What-is-calibration?","page":"Mathematical Background","title":"What is calibration?","text":"","category":"section"},{"location":"mathematical_background/","page":"Mathematical Background","title":"Mathematical Background","text":"Calibration gives us a way to quantify whether the uncertainty in a series of predictions is faithful to the distribution of prediction errors. Given a series of predicted probability distributions p_hatxi_i and corresponding measurements xi_i, we call the predictions (marginally) calibrated if the predicted  probabilities match the observed frequencies of events. For example, if each prediction is a univariate normal distribution, we expect about 68% of the observations to fall within one standard deviation of the prediction.","category":"page"},{"location":"mathematical_background/","page":"Mathematical Background","title":"Mathematical Background","text":"One way to write this mathematically is","category":"page"},{"location":"mathematical_background/","page":"Mathematical Background","title":"Mathematical Background","text":"mathitPrleft( xi_i leq mathitcdf(p_hatxi_i rho ) right) approx rho","category":"page"},{"location":"mathematical_background/","page":"Mathematical Background","title":"Mathematical Background","text":"i.e. defining calibration through the cumulative density function mathitcdf(p_(cdot) rho) of the distribution p_(cdot) evaluated at probability rho.[cdf] Notably, however, the cumulative density function is not defined for multivariate distributions. For this reason we introduce The Central Prediction Set approach implemented in this package.","category":"page"},{"location":"mathematical_background/#The-Central-Prediction-Set","page":"Mathematical Background","title":"The Central Prediction Set","text":"","category":"section"},{"location":"mathematical_background/","page":"Mathematical Background","title":"Mathematical Background","text":"Let us now construct a simple scheme how we can efficiently construct centered prediction sets mathcalP for multivariate normal distributions D = mathcalN(mu Sigma) with mu in mathbbR^d and Sigma = Sigma^top in mathbbR^d times d. These prediction sets will be centered in the sense that they are constructed by growing them symmetrically outwards from the \"center\" of the distribution.","category":"page"},{"location":"mathematical_background/","page":"Mathematical Background","title":"Mathematical Background","text":"Our constructed is based on two key observations: First, any normal distribution can be \"diagonalized\" without fundamentally changing the probability density by considering a new distribution tildeD = mathcalN(tildemu tildeSigma) where tildeSigma = QSigma Q^top = diag(tildesigma_1^2 dots tildesigma_d^2) with a rotation matrix Q s.t. Q_2=0, and appropriately rotated samples tildex = Qx Further, the probability set mathcalP(tildeD p) for diagonalized normal distributions can simply be constructed by considering each dimension individually, with","category":"page"},{"location":"mathematical_background/","page":"Mathematical Background","title":"Mathematical Background","text":"mathcalP(tildeD p) = left tildex mid tildex_i in mathcalPleft(mathcalN(tildemu_i tildesigma_i^2) p^1d right) forall i in 1d right","category":"page"},{"location":"mathematical_background/","page":"Mathematical Background","title":"Mathematical Background","text":"where we can construct mathcalP for the one dimensional case. We can do this by again picking a centralized construction, although any other constructions, e.g. based on the cumulative density function, are also valid. Using the central construction, we can normalize the problem by the \"length scale\" of each univariate normal and compute","category":"page"},{"location":"mathematical_background/","page":"Mathematical Background","title":"Mathematical Background","text":"tildex_i in mathcalPleft(mathcalN(tildemu_i tildesigma_i^2) p^1d right) Leftrightarrow left fractildex_i - tildemu_itildesigma_i right  quantileleft(mathcalN(0 1) fracp^1d2 + frac12right)","category":"page"},{"location":"mathematical_background/","page":"Mathematical Background","title":"Mathematical Background","text":"Finally, we use the above construction to construct the final prediction set   mathcalP(D p) = left x mid Qx in P(tildeD p)right that is easy and efficient to check for any x.","category":"page"},{"location":"mathematical_background/","page":"Mathematical Background","title":"Mathematical Background","text":"Further, we can also define the inverse probability set function mathcalP^-1(D x) = p, i.e. the first probability p such that mathcalP(D p) includes x, by checking the maximum deviation (normalized by length scale) in each dimension, i.e.","category":"page"},{"location":"mathematical_background/","page":"Mathematical Background","title":"Mathematical Background","text":"mathcalP^-1(tildeD x) = max_i left(cdfleft(mathcalN(0 1) left fractildex_i - tildemu_itildesigma_i rightright) - frac12right) cdot 2","category":"page"},{"location":"mathematical_background/#Calibration-and-Sharpness","page":"Mathematical Background","title":"Calibration and Sharpness","text":"","category":"section"},{"location":"mathematical_background/","page":"Mathematical Background","title":"Mathematical Background","text":"Further, we note that a series of predictions may be perfectly calibrated, but still be \"bad\" in the sense that each prediction has large uncertainty. Therefore, we must also measure sharpness, a measure of the \"conciseness\" of the predictions. A good prediction is therefore well calibrated and sharp at the same time. We present an example in Measuring Sharpness. We refer to Gneiting and Katzfuss (2014) for an overview of the trade off between calibration and sharpness.","category":"page"},{"location":"mathematical_background/","page":"Mathematical Background","title":"Mathematical Background","text":"Gneiting, T. and Katzfuss, M. (2014). Probabilistic Forecasting. Annual Review of Statistics and Its Application.\n\n\n\n","category":"page"},{"location":"mathematical_background/","page":"Mathematical Background","title":"Mathematical Background","text":"[cdf]: If we only have samples of xi_i, we may replace mathitcdf with the /empirical/ mathitcdf.","category":"page"},{"location":"#MvNormalCalibration.jl","page":"MvNormalCalibration.jl","title":"MvNormalCalibration.jl","text":"","category":"section"},{"location":"","page":"MvNormalCalibration.jl","title":"MvNormalCalibration.jl","text":"This package implements computing calibration and a measure of sharpness, specifically for uni- and multivariate normal distributions, i.e. \"Gaussians\". This page will present a brief usage example and rationale for the  computecalibration and sharpness functions. We also provide some Mathematical Background on calibration and an API Reference.","category":"page"},{"location":"#A-Brief-Usage-Example","page":"MvNormalCalibration.jl","title":"A Brief Usage Example","text":"","category":"section"},{"location":"","page":"MvNormalCalibration.jl","title":"MvNormalCalibration.jl","text":"Before we go into the mathematical background, here's a quick usage example for measuring calibration using computecalibration. In this example, we make a constant 2D prediction; however, the true distribution slightly  changes at every time step.","category":"page"},{"location":"","page":"MvNormalCalibration.jl","title":"MvNormalCalibration.jl","text":"using Distributions\nusing MvNormalCalibration: computecalibration, sharpness\nusing LinearAlgebra: I, hermitianpart\n\nnobs = 500\n\n# We predict that every observation is a zero mean, iso-normal...\npredictions = [MvNormal(zeros(2), I(2)) for _ in 1:nobs]\n# ... but actually, each observation has nonzero mean and small covariate terms.\n# Note: Usually we don't know the true distributions.\ntrue_distributions = [MvNormal((rand(2).-0.5)./10, I(2) + hermitianpart(rand(2,2)))\n                      for _ in 1:nobs]\nobservations = rand.(true_distributions)\n\n# Let's compute calibration for our predictions.\n(; pvals, calibrationvals) = computecalibration(predictions, observations)\n\n# As expected, there is a significant gap between theoretical and actual calibration values.\n@info \"\"\"\nMaximum calibration discrepancy for (somewhat inaccurate) predictions\n$(maximum(abs, pvals - calibrationvals) |> x->round(x; sigdigits=2))\n\"\"\"\n\n# However, if we had managed to predict each distribution accurately, things would look better.\n(; pvals, calibrationvals) = computecalibration(true_distributions, observations)\n@info \"\"\"\nand for true distributions\n$(maximum(abs, pvals - calibrationvals) |> x->round(x; sigdigits=2)).\n\"\"\"","category":"page"},{"location":"#Plotting-calibration-results","page":"MvNormalCalibration.jl","title":"Plotting calibration results","text":"","category":"section"},{"location":"","page":"MvNormalCalibration.jl","title":"MvNormalCalibration.jl","text":"It can also be useful to plot the resulting calibration values again the theoretical values. Indeed, for perfect calibration, we expect the theoretical coverage probabilities (pvals) to exactly match the observed coverage probabilities (calibrationvals) computed by computecalibration. Let's compute and plot calibration for overly optimistic predictions (variance too small, assumes zero-mean), for \"perfect\" predictions, and for overly pessimistic predictions.","category":"page"},{"location":"","page":"MvNormalCalibration.jl","title":"MvNormalCalibration.jl","text":"using Distributions, MvNormalCalibration, LinearAlgebra, Plots\nplot_kwargs = (; label=\"Predictions\", aspect_ratio=:equal, xlabel=\"Theoretical coverage\",\n                 ylabel=\"Observed coverage\", xlims=(0,1), ylims=(0,1), size=(750, 280))\nnobs = 500\noptimistic_predictions  = [MvNormal(zeros(2), I(2)) for _ in 1:nobs]\ntrue_distributions      = [MvNormal(rand(2)./10, I(2) + hermitianpart(rand(2,2)))\n                           for _ in 1:nobs]\npessimistic_predictions = [1.3^2 * I(2)] .* optimistic_predictions\nobservations = rand.(true_distributions)\n\np1 = plot(computecalibration(optimistic_predictions, observations)...;\n          title=\"Overconfident\", plot_kwargs...)\np2 = plot(computecalibration(true_distributions, observations)...;\n          title=\"Well calibrated\", plot_kwargs...)\np3 = plot(computecalibration(pessimistic_predictions, observations)...;\n          title=\"Pessimistic\", plot_kwargs...)\nfor p in [p1, p2, p3]; plot!(p, [0,1], [0,1]; label=\"Ground Truth\"); end\nplot(p1, p2, p3; layout=(1,3))","category":"page"},{"location":"#Measuring-Sharpness","page":"MvNormalCalibration.jl","title":"Measuring Sharpness","text":"","category":"section"},{"location":"","page":"MvNormalCalibration.jl","title":"MvNormalCalibration.jl","text":"Calibration paints only half the picture. We can have well calibrated, but still imprecise predictions if they have (unnecessarily) large uncertainty, i.e. they are not sharp. Here is an example of such a setup, and how to compute and interpret sharpness.","category":"page"},{"location":"","page":"MvNormalCalibration.jl","title":"MvNormalCalibration.jl","text":"using Distributions, MvNormalCalibration, LinearAlgebra\npreds_poor  = [MvNormal(zeros(2), I(2)) for _ in 1:1_000]\npreds_sharp = [MvNormal( rand(D), (0.1^2).*I(2)) for D in preds_poor]\nobservations = rand.(preds_sharp)\n@info \"\"\"Average sharpness of poor predictions:\n$(mean(sharpness.(preds_poor)) |> x->round(x; sigdigits=2))\n\"\"\"\n@info \"\"\"Average sharpness of sharp predictions:\n$(mean(sharpness.(preds_sharp)) |> x->round(x; sigdigits=2))\n\"\"\"","category":"page"},{"location":"","page":"MvNormalCalibration.jl","title":"MvNormalCalibration.jl","text":"Indeed we see that the one set of predictions has one hundred times lower uncertainty than the other. But do they both predict the observations equally well? Let's compare both predictions' calibration and sharpness. We will see that both predictions have almost  identical calibration curves, despite their difference in uncertainty.","category":"page"},{"location":"#Comparing-predictions-via-calibration-and-sharpness","page":"MvNormalCalibration.jl","title":"Comparing predictions via calibration and sharpness","text":"","category":"section"},{"location":"","page":"MvNormalCalibration.jl","title":"MvNormalCalibration.jl","text":"using Distributions, MvNormalCalibration, Plots, LinearAlgebra\nimport Random: seed!\npreds_poor  = [MvNormal(zeros(2), I(2)) for _ in 1:1_000]\npreds_sharp = [MvNormal( rand(D), (0.1^2).*I(2)) for D in preds_poor]\n\nobservations = rand.(preds_sharp)\npvals = 0:0.01:1\ncalibrationvals_poor  = computecalibration(preds_poor, observations; pvals)[:calibrationvals]\ncalibrationvals_sharp = computecalibration(preds_sharp, observations; pvals)[:calibrationvals]\n\nplt1 = plot(pvals, calibrationvals_poor;\n            label=\"Poor (constant) predictions\", title=\"Calibration curve\",\n            aspect_ratio=:equal, xlabel=\"Theoretical coverage\", ylabel=\"Observed coverage\",\n            xlims=(0,1), ylims=(0,1), size=(700, 360))\nplot!(pvals, calibrationvals_sharp; label=\"Sharp predictions\")\n\nseed!(1)\nxs = LinRange(-3, 3, 100)\nplt2 = plot(xs, x->pdf(Normal(), x);\n            label=\"Poor (constant) predictions\", title=\"Predictions\", xlims=(-3, 3),ylims=(0, 6), aspect_ratio=:equal, xlabel=raw\"$x$\", ylabel=\"Probability \"*raw\"$pdf(x)$\")\nfor i in 1:6\n   D = Normal(randn(), 0.1)\n   xs_=LinRange(mean(D)-5*std(D), mean(D)+5*std(D), 100)\n   plot!(plt2, xs_, x->pdf(D, x); color=2, label=(i==1 ? \"Sharp predictions (i=1,2,..)\" : nothing))\nend\nplot(plt1, plt2)","category":"page"},{"location":"","page":"MvNormalCalibration.jl","title":"MvNormalCalibration.jl","text":"We can see that both sets of predictions have practically identical calibration curves, but  extremely different sharpness – i.e. the sharp predictions are much more precise for each sample.","category":"page"}]
}
